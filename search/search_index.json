{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MUBen Documentation This is the documentation for MUBen : Mulecular Uncertainty Benchmark. The code is built to expose implementation details as much as possible and be easily extendable. Questions and suggestions are welcome if you find any issues while using our code. About MUBen is a benchmark that aims to investigate the performance of uncertainty quantification (UQ) methods built upon backbone molecular representation models. It implements 6 backbone models (4 pre-trained and 2 non-pre-trained), 8 UQ methods (8 compatible for classification and 6 for regression), and 14 datasets from MoleculeNet (8 for classification and 6 for regression). We are actively expanding the benchmark to include more backbones, UQ methods and datasets. This is an arduous task, and we welcome contribution or collaboration in any form. Backbones Backbone Models Paper Official Repo Our Implementation Pre-Trained Backbones ChemBERTa link link link GROVER link link link Uni-Mol link link link TorchMD-NET Architecture ; Pre-training link link Non-Pre-Trained Backbones DNN - - link GIN link pyg link Uncertainty Quantification Methods UQ Method Classification Regression Paper Deterministic \u2705\ufe0e \u2705\ufe0e - Temperature Scaling \u2705\ufe0e - link Focal Loss \u2705\ufe0e - link Deep Ensembles \u2705\ufe0e \u2705\ufe0e link SWAG \u2705\ufe0e \u2705\ufe0e link Bayes by Backprop \u2705\ufe0e \u2705\ufe0e link SGLD \u2705\ufe0e \u2705\ufe0e link MC Dropout \u2705\ufe0e \u2705\ufe0e link Data Please check MoleculeNet for a detailed description. We use a subset of the MoleculeNet benckmark, including BBBP, Tox21, ToxCast, SIDER, ClinTox, BACE, MUV, HIV, ESOL, FreeSolv, Lipophilicity, QM7, QM8, QM9. Data A set of partitioned datasets are already included in this repo. You can find them under the ./data/ folder: [ scaffold split ]; [ random split ]. We utilize the datasets prepared by Uni-Mol . You find the data here or directly download it through this link . We place the unzipped files into ./data/UniMol by default. For convenience, you are suggested to rename the qm7dft , qm8dft , and qm9dft folders to qm7 , qm8 , and qm9 . Afterwards, you can transfer the dataset format into ours by running PYTHONPATH=\".\" python ./assist/dataset_build_from_unimol.py suppose you are in the project root directory. You can specify the input (Uni-Mol) and output data directories with --unimol_data_dir and --output_dir arguments. The script will convert all datasets by default (excluding PCBA). If you want to specify a subset of datasets, you can specify the argument --dataset_names with the target dataset names with lowercase letters. Notice : If you would like to run the Uni-Mol model, you are suggested to keep the original UniMol data as we will use the pre-defined molecule conformations. Otherwise, it is safe to remove the original data. Other Options If you do not want to use Uni-Mol data, you can try the scripts within the legacy folder, including build_dgllife_datasets.py , and build_qm[7,8,9]_dataset.py . Notice that this may result in training/validation/test partitions different from what is being used in our experiments. Using Customized Datasets If you want to test the UQ methods on your own dataset, you can use pandas.DataFrame structure with the following keys: { \"smiles\": list of `str`, \"labels\": list of list of int/float, \"masks\": list of list of int/float (with values within {0,1}) } and store them as train.csv , valid.csv , and test.csv files. mask=1 indicates the existence informative label at the position and mask=0 indicates missing label. You can check the prepared datasets included in our program for reference. You are recommended to put the dataset files in the ./data/file/<dataset name> directory, but you can of course choose your favorite location and specify the --data_folder argument. The .csv files should be accompanied by a meta.json file within the same directory. It stores some constant dataset properties, e.g. , task_type (classification or regression), n_tasks , or classes ( [0,1] for all our classification datasets). For the customized dataset, one required property is the eval_metric for validation and test ( e.g. , roc-auc, rmse, etc. ) since it is not specified in the macro file. Please refer to ./assist/dataset_build_roe.py for an example (unfortunately, we are not allowed to release the dataset). Run A simple demo of running our project can be found at ./demo/demo.ipynb . To run each of the four backbone models with uncertainty estimation methods, you can check the run_*.py files in the root directory. Example shell scripts are provided in the ./scripts folder as .sh files. You can use them through ./scripts/run_dnn_rdkit.sh <CUDA_VISIBLE_DEVICES> as an example. Notice that we need to comment out the variables train_on_<dataset name> in the .sh files to skip training on the corresponding datasets. Setting their value to false does not work . Another way of specifying arguments is through the .json scripts, for example: PYTHONPATH=\".\" CUDA_VISIBLE_DEVICES=0 python ./run/dnn.py ./scripts/config_dnn.json This approach could be helpful for debugging the code through vscode. To get a detailed description of each argument, you can use --help : PYTHONPATH=\".\" python ./run/dnn.py --help Logging and WandB By default, this project uses local logging files ( *.log ) and WandB to track training status. The log files are stored as ./logs/<dataset>/<model>/<uncertainty>/<running_time>.log . You can change the file path by specifying the --log_path argument, or disable log saving by setting --log_path=\"disabled\" . To use WandB, you first need to register an account and sign in on your machine with wandb login . If you are running your code on a public device, you can instead use program-wise signing in by specifying the --wandb_api_key argument while running our code. You can find your API key in your browser here: https://wandb.ai/authorize. To disable WandB, use --disable_wandb [true] . By default, we use MUBen-<dataset> as WandB project name and <model>-<uncertainty> as the model name. You can change this behavior by specifying the --wandb_project and --wandb_name arguments. Data Loading The progress will automatically create the necessary features (molecular descriptors) required by backbone models from the SMILES strings if they are loaded properly. The processed features are stored in the <bottom-level data folder>/processed/ directory as <train/valid/test>.pt files by default, and will be automatically loaded the next time you apply the same backbone model on the same dataset. You can change this behavior with --disable_dataset_saving for disabling dataset saving or --ignore_preprocessed_dataset for not loading from the saved (processed) dataset. Constructing Morgan fingerprint, RDKit features or 3D conformations for Uni-Mol may take a while. You can accelerate this process by utilizing multiple threads --num_preprocess_workers=n>1 (default is 8). For 3D conformations, we directly take advantage of the results from Uni-Mol but still keep the choice of generating them by ourselves if the Uni-Mol data files are not found. Calculating Metrics During training, we only calculate metrics necessary for early stopping and simple prediction performance evaluation. To get other metrics, you need to use the ./assist/results_get_metrics.py file. Specifically, you need to save the model predictions by not setting --disable_dataset_saving . The results are saved as ./<result_folder>/<dataset_name>/<model_name>/<uncertainty_method>/seed-<seed>/preds/<test_idx>.pt files. When the training is finished, you can run the ./assist/results_get_metrics.py file to generate all metrics for your model predictions. For example: PYTHONPATH=\".\" python ./assist/results_get_metrics.py ./scripts/config_metrics.json Make sure the hyper-parameters in the configuration file are updated to your needs. The metrics will be saved in the ./<result_folder>/RESULTS/<model_name>-<dataset_name>.csv files. ~~Notice that these files already exist in the repo if you keep the default --result_folder=./output argument and you need to check whether it is updated to reveal your experiment results.~~ Results We provided a more comprehensive copy of our experiment results here that are presented in the tables in our paper's appendix. We hope it can ease some effort if you want to further analyze the behavior of our backbone models and uncertainty quantification methods. Ongoing Works Active Learning We are developing code to integrate active learning into the pipeline. Specifically, we assume we have a small set of labeled data points ( --n_init_instances ) at the beginning. Within each active learning iteration, we use the labeled dataset to fine-tune the model parameters and select a batch of data points ( --n_al_select ) from the unlabeled set with the least predicted certainty ( i.e. , max predicted entropy for classification and max predicted variance for regression). The process is repeated for several loops ( --n_al_loops ), and the intermediate performance is tracked. The code is still under construction and currently is only available under the dev branch . In addition, several points are worth attention: Currently, only DNN and ChemBERTa backbones are supported ( ./run/dnn_al.py and ./run/chemberta_al.py ). Migrating AL to other backbones is not difficult but requires updating some Trainer functions if they are reloaded. To enable active learning, make sure you set --enable_active_learning to true . Currently, Deep Ensembles is not supported for AL. We cannot guarantee the correctness of our implementation. If you notice any abnormalities in the code, please do not hesitate to post an issue. One example is python ./run/dnn_al.py \\ --enable_active_learning \\ --n_init_instances 100 \\ --n_al_loops 20 \\ --n_al_select 20 \\ # other model and training hyper-parameters... Citation If you find our work helpful, please consider citing it as @misc{li2023muben, title={MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction}, author={Yinghao Li and Lingkai Kong and Yuanqi Du and Yue Yu and Yuchen Zhuang and Wenhao Mu and Chao Zhang}, year={2023}, eprint={2306.10060}, archivePrefix={arXiv}, primaryClass={physics.chem-ph} }","title":"About"},{"location":"#muben-documentation","text":"This is the documentation for MUBen : Mulecular Uncertainty Benchmark. The code is built to expose implementation details as much as possible and be easily extendable. Questions and suggestions are welcome if you find any issues while using our code.","title":"MUBen Documentation"},{"location":"#about","text":"MUBen is a benchmark that aims to investigate the performance of uncertainty quantification (UQ) methods built upon backbone molecular representation models. It implements 6 backbone models (4 pre-trained and 2 non-pre-trained), 8 UQ methods (8 compatible for classification and 6 for regression), and 14 datasets from MoleculeNet (8 for classification and 6 for regression). We are actively expanding the benchmark to include more backbones, UQ methods and datasets. This is an arduous task, and we welcome contribution or collaboration in any form.","title":"About"},{"location":"#backbones","text":"Backbone Models Paper Official Repo Our Implementation Pre-Trained Backbones ChemBERTa link link link GROVER link link link Uni-Mol link link link TorchMD-NET Architecture ; Pre-training link link Non-Pre-Trained Backbones DNN - - link GIN link pyg link","title":"Backbones"},{"location":"#uncertainty-quantification-methods","text":"UQ Method Classification Regression Paper Deterministic \u2705\ufe0e \u2705\ufe0e - Temperature Scaling \u2705\ufe0e - link Focal Loss \u2705\ufe0e - link Deep Ensembles \u2705\ufe0e \u2705\ufe0e link SWAG \u2705\ufe0e \u2705\ufe0e link Bayes by Backprop \u2705\ufe0e \u2705\ufe0e link SGLD \u2705\ufe0e \u2705\ufe0e link MC Dropout \u2705\ufe0e \u2705\ufe0e link","title":"Uncertainty Quantification Methods"},{"location":"#data","text":"Please check MoleculeNet for a detailed description. We use a subset of the MoleculeNet benckmark, including BBBP, Tox21, ToxCast, SIDER, ClinTox, BACE, MUV, HIV, ESOL, FreeSolv, Lipophilicity, QM7, QM8, QM9.","title":"Data"},{"location":"#data_1","text":"A set of partitioned datasets are already included in this repo. You can find them under the ./data/ folder: [ scaffold split ]; [ random split ]. We utilize the datasets prepared by Uni-Mol . You find the data here or directly download it through this link . We place the unzipped files into ./data/UniMol by default. For convenience, you are suggested to rename the qm7dft , qm8dft , and qm9dft folders to qm7 , qm8 , and qm9 . Afterwards, you can transfer the dataset format into ours by running PYTHONPATH=\".\" python ./assist/dataset_build_from_unimol.py suppose you are in the project root directory. You can specify the input (Uni-Mol) and output data directories with --unimol_data_dir and --output_dir arguments. The script will convert all datasets by default (excluding PCBA). If you want to specify a subset of datasets, you can specify the argument --dataset_names with the target dataset names with lowercase letters. Notice : If you would like to run the Uni-Mol model, you are suggested to keep the original UniMol data as we will use the pre-defined molecule conformations. Otherwise, it is safe to remove the original data.","title":"Data"},{"location":"#other-options","text":"If you do not want to use Uni-Mol data, you can try the scripts within the legacy folder, including build_dgllife_datasets.py , and build_qm[7,8,9]_dataset.py . Notice that this may result in training/validation/test partitions different from what is being used in our experiments.","title":"Other Options"},{"location":"#using-customized-datasets","text":"If you want to test the UQ methods on your own dataset, you can use pandas.DataFrame structure with the following keys: { \"smiles\": list of `str`, \"labels\": list of list of int/float, \"masks\": list of list of int/float (with values within {0,1}) } and store them as train.csv , valid.csv , and test.csv files. mask=1 indicates the existence informative label at the position and mask=0 indicates missing label. You can check the prepared datasets included in our program for reference. You are recommended to put the dataset files in the ./data/file/<dataset name> directory, but you can of course choose your favorite location and specify the --data_folder argument. The .csv files should be accompanied by a meta.json file within the same directory. It stores some constant dataset properties, e.g. , task_type (classification or regression), n_tasks , or classes ( [0,1] for all our classification datasets). For the customized dataset, one required property is the eval_metric for validation and test ( e.g. , roc-auc, rmse, etc. ) since it is not specified in the macro file. Please refer to ./assist/dataset_build_roe.py for an example (unfortunately, we are not allowed to release the dataset).","title":"Using Customized Datasets"},{"location":"#run","text":"A simple demo of running our project can be found at ./demo/demo.ipynb . To run each of the four backbone models with uncertainty estimation methods, you can check the run_*.py files in the root directory. Example shell scripts are provided in the ./scripts folder as .sh files. You can use them through ./scripts/run_dnn_rdkit.sh <CUDA_VISIBLE_DEVICES> as an example. Notice that we need to comment out the variables train_on_<dataset name> in the .sh files to skip training on the corresponding datasets. Setting their value to false does not work . Another way of specifying arguments is through the .json scripts, for example: PYTHONPATH=\".\" CUDA_VISIBLE_DEVICES=0 python ./run/dnn.py ./scripts/config_dnn.json This approach could be helpful for debugging the code through vscode. To get a detailed description of each argument, you can use --help : PYTHONPATH=\".\" python ./run/dnn.py --help","title":"Run"},{"location":"#logging-and-wandb","text":"By default, this project uses local logging files ( *.log ) and WandB to track training status. The log files are stored as ./logs/<dataset>/<model>/<uncertainty>/<running_time>.log . You can change the file path by specifying the --log_path argument, or disable log saving by setting --log_path=\"disabled\" . To use WandB, you first need to register an account and sign in on your machine with wandb login . If you are running your code on a public device, you can instead use program-wise signing in by specifying the --wandb_api_key argument while running our code. You can find your API key in your browser here: https://wandb.ai/authorize. To disable WandB, use --disable_wandb [true] . By default, we use MUBen-<dataset> as WandB project name and <model>-<uncertainty> as the model name. You can change this behavior by specifying the --wandb_project and --wandb_name arguments.","title":"Logging and WandB"},{"location":"#data-loading","text":"The progress will automatically create the necessary features (molecular descriptors) required by backbone models from the SMILES strings if they are loaded properly. The processed features are stored in the <bottom-level data folder>/processed/ directory as <train/valid/test>.pt files by default, and will be automatically loaded the next time you apply the same backbone model on the same dataset. You can change this behavior with --disable_dataset_saving for disabling dataset saving or --ignore_preprocessed_dataset for not loading from the saved (processed) dataset. Constructing Morgan fingerprint, RDKit features or 3D conformations for Uni-Mol may take a while. You can accelerate this process by utilizing multiple threads --num_preprocess_workers=n>1 (default is 8). For 3D conformations, we directly take advantage of the results from Uni-Mol but still keep the choice of generating them by ourselves if the Uni-Mol data files are not found.","title":"Data Loading"},{"location":"#calculating-metrics","text":"During training, we only calculate metrics necessary for early stopping and simple prediction performance evaluation. To get other metrics, you need to use the ./assist/results_get_metrics.py file. Specifically, you need to save the model predictions by not setting --disable_dataset_saving . The results are saved as ./<result_folder>/<dataset_name>/<model_name>/<uncertainty_method>/seed-<seed>/preds/<test_idx>.pt files. When the training is finished, you can run the ./assist/results_get_metrics.py file to generate all metrics for your model predictions. For example: PYTHONPATH=\".\" python ./assist/results_get_metrics.py ./scripts/config_metrics.json Make sure the hyper-parameters in the configuration file are updated to your needs. The metrics will be saved in the ./<result_folder>/RESULTS/<model_name>-<dataset_name>.csv files. ~~Notice that these files already exist in the repo if you keep the default --result_folder=./output argument and you need to check whether it is updated to reveal your experiment results.~~","title":"Calculating Metrics"},{"location":"#results","text":"We provided a more comprehensive copy of our experiment results here that are presented in the tables in our paper's appendix. We hope it can ease some effort if you want to further analyze the behavior of our backbone models and uncertainty quantification methods.","title":"Results"},{"location":"#ongoing-works","text":"","title":"Ongoing Works"},{"location":"#active-learning","text":"We are developing code to integrate active learning into the pipeline. Specifically, we assume we have a small set of labeled data points ( --n_init_instances ) at the beginning. Within each active learning iteration, we use the labeled dataset to fine-tune the model parameters and select a batch of data points ( --n_al_select ) from the unlabeled set with the least predicted certainty ( i.e. , max predicted entropy for classification and max predicted variance for regression). The process is repeated for several loops ( --n_al_loops ), and the intermediate performance is tracked. The code is still under construction and currently is only available under the dev branch . In addition, several points are worth attention: Currently, only DNN and ChemBERTa backbones are supported ( ./run/dnn_al.py and ./run/chemberta_al.py ). Migrating AL to other backbones is not difficult but requires updating some Trainer functions if they are reloaded. To enable active learning, make sure you set --enable_active_learning to true . Currently, Deep Ensembles is not supported for AL. We cannot guarantee the correctness of our implementation. If you notice any abnormalities in the code, please do not hesitate to post an issue. One example is python ./run/dnn_al.py \\ --enable_active_learning \\ --n_init_instances 100 \\ --n_al_loops 20 \\ --n_al_select 20 \\ # other model and training hyper-parameters...","title":"Active Learning"},{"location":"#citation","text":"If you find our work helpful, please consider citing it as @misc{li2023muben, title={MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction}, author={Yinghao Li and Lingkai Kong and Yuanqi Du and Yue Yu and Yuchen Zhuang and Wenhao Mu and Chao Zhang}, year={2023}, eprint={2306.10060}, archivePrefix={arXiv}, primaryClass={physics.chem-ph} }","title":"Citation"},{"location":"started/","text":"Get Started In the following, we give a brief introduction to our experiment pipeline, which might be helpful if you would like to replicate our results or extend MUBen to other datasets, backbone models, or uncertainty quantification (UQ) methods. You can also find the example in this Jupyter Notebook . Preparation Currently, we do not provide Python wheels for installation as we aim to expose as many implementation details as possible. You are recommended to directly use or modify the source code to conduct your experiments. To do that, you can first fork the project and clone it to local with git clone . git clone https://github.com/<your GitHub username>/MUBen.git Or, you can directly clone this repository with git or GitHub CLI suppose you do not intend to do change tracking. # clone with git git clone https://github.com/Yinghao-Li/MUBen.git # or, you can clone with GitHub CLI gh repo clone Yinghao-Li/MUBen The following operations assume that you are already in the project root directory MUBen/ . Requirements Our code is developed with Python 3.10 . Notice that it may not work with Python < 3.9 . It is recommended to create a new conda environment for this project. conda create -n muben python=3.10 The required packages are listed in requirements.txt . It is recommended to install these dependencies with pip install as conda install may sometimes encounter dependency resolution issue. conda activate muben pip install -r requirements.txt Docker Alternatively, you can run this project in a docker container. You can build your image through docker build -t muben ./docker and run your container in the terminal with docker run --gpus all -it --rm muben External dependencies The backbone models GROVER and Uni-Mol require loading pre-trained model checkpoints. The GROVER-base checkpoint is available at GROVER's project repo or can be directly downloaded through this link . Unzip the downloaded .tar.gz file to get the .pt checkpoint. The Uni-Mol checkpoint is available at Uni-Mol's project repo or can be directly downloaded through this link . By default, the code will look for the models at locations ./models/grover_base.pt and ./models/unimol_base.pt , respectively. You need to specify the --checkpoint_path argument if you prefer other locations and checkpoint names. A simple example In this demonstration, we'll guide you through foundational training and testing of MUBen using the BBBP dataset as a minimal example. We've chosen the DNN as our backbone model because it is both efficient and offers satisfactory performance. For uncertainty quantification (UQ), we'll evaluate both the Deterministic method (referred to as \"none\" within MUBen) and Temperature Scaling. While the procedures for other backbone models, UQ methods, or datasets are largely similar, you can explore specific variations by referring to API documentation. Importing packages The first step is to import all the necessary packages from the MUBen source code that defines the datasets, backbone models, UQ methods, and trainers. import logging import wandb from transformers import set_seed from muben.utils.selectors import configure_selector, dataset_selector, model_selector from muben.train import Trainer from muben.utils.io import set_logging # initialize logger logger = logging.getLogger(__name__) Deterministic method -- training We first train the DNN model Deterministically. That is, we do not apply any UQ method to the model. Instead, we directly use its post-output-activation probabilities as its estimated reliability to its prediction. Here we pass necessary hyper-parameters to the configuration to control the training process. # Set up the logging format and random seed. # We do not use wandb for this demo, so we set its mode to \"disabled\". set_logging() set_seed(42) wandb.init(mode=\"disabled\",) # Select the classes based on the descriptor type. # DNN uses RDKit features, so we set the descriptor type to RDKit and select configuration, dataset, # and model classes according to it. descriptor_type = \"RDKit\" config_class = configure_selector(descriptor_type) dataset_class = dataset_selector(descriptor_type) model_class = model_selector(descriptor_type) # Specify the configuration of the experiment. # Notice that although we directly edit the config object here, a more appropriate way of doing this is # passing arguments through the shell or json scripts when we are running the experiments through the terminal. config = config_class() config.model_name = \"DNN\" config.feature_type = \"rdkit\" config.data_folder = \"../data/files/\" config.dataset_name = \"bbbp\" config.result_folder = \"../output-demo/\" config.uncertainty_method = \"none\" # here \"none\" refers to \"Deterministic\" config.retrain_model = True # We only train the model for a few epochs for the demo. config.n_epochs = 50 # activate training timer config.time_training = True # Post initialization of the arguments. config.__post_init__() # Load dataset metadata, validate the arguments, and log the configuration. _ = config.get_meta().validate().log() The configuration details are printed out in your terminal by calling config.log() . Similar to configuration, we automatically infer the dataset and collate function classes according to the descriptor type we set above. Then, we initialize the training, validation, and test datasets. # Load and process the training, validation and test datasets dataset_class, collator_class = dataset_selector(descriptor_type) training_dataset = dataset_class().prepare(config=config, partition=\"train\") valid_dataset = dataset_class().prepare(config=config, partition=\"valid\") test_dataset = dataset_class().prepare(config=config, partition=\"test\") Afterward, we can initialize the trainer and model with our configuration. model_selector automatically detects the model type according to the descriptor. In this case, DNN is the selected model. Then, the trainer initializes the model with arguments defined in the configuration. # Inintialized the trainer with the configuration and datasets # Inintialized the trainer with the configuration and datasets trainer = Trainer( config=config, model_class=model_selector(descriptor_type), training_dataset=training_dataset, valid_dataset=valid_dataset, test_dataset=test_dataset, collate_fn=collator_class(config), ).initialize(config=config) Once the trainer is initialized, we can call trainer.run() to do the training. # Run the training, validation and test process. # The model checkpoint and predicted results will be automatically saved in the specified output folder. trainer.run() Temperature Scaling -- training Training the DNN model with Temperature Scaling is basically identical to training with the Deterministic method. The only difference is that we need to define the uncertainty_method in config as \"TemperatureScaling\" instead of \"none\" . wandb.init(mode=\"disabled\",) # Change some configuration items. config.uncertainty_method = \"TemperatureScaling\" config.retrain_model = False config.n_ts_epochs = 10 # number of epochs for training the temperature scaling layer. config.__post_init__() _ = config.validate().log() # Re-inintialized the trainer with the updated configuration. # The datasets are not changed. trainer = Trainer( config=config, model_class=model_selector(descriptor_type), training_dataset=training_dataset, valid_dataset=valid_dataset, test_dataset=test_dataset, collate_fn=collator_class(config), ).initialize(config=config) # Run the training, validation and test process. # The trainer will load the model checkpoint from the Deterministic run and # continue training the temperature scaling layer. # Notice that not all UQ methods support continued training. For example, BBP requires training from scratch. trainer.run() Evaluation Here, we provide a simplified version of metric calculation. Please check <project root>/assist/result_get_metrics.py for the full function. import os.path as osp import pandas as pd from muben.utils.metrics import classification_metrics from muben.utils.io import load_results # Define the path to the predicted results. \"det\" stands for \"Deterministic\"; \"ts\" stands for \"Temperature Scaling\". det_result = osp.join( config.result_folder, config.dataset_name, f\"{config.model_name}-{config.feature_type}\", \"none\", f\"seed-{config.seed}\", \"preds\", \"0.pt\" ) ts_result = osp.join( config.result_folder, config.dataset_name, f\"{config.model_name}-{config.feature_type}\", \"TemperatureScaling\", f\"seed-{config.seed}\", \"preds\", \"0.pt\" ) # Load the predicted results. det_preds, _, lbs, masks = load_results([det_result]) ts_preds, _, _, _ = load_results([ts_result]) # Calculate the metrics. det_metrics = classification_metrics(det_preds, lbs, masks) ts_metrics = classification_metrics(ts_preds, lbs, masks) det_metrics = {k: v['macro-avg'] for k, v in det_metrics.items()} ts_metrics = {k: v['macro-avg'] for k, v in ts_metrics.items()} # Present the results in a dataframe. det_metrics_df = pd.DataFrame({\"Deterministic\": det_metrics, \"TemperatureScaling\": ts_metrics}) print(det_metrics_df.T) The result will be presented as a table the columns being metrics and rows being the UQ method.","title":"Get Started"},{"location":"started/#get-started","text":"In the following, we give a brief introduction to our experiment pipeline, which might be helpful if you would like to replicate our results or extend MUBen to other datasets, backbone models, or uncertainty quantification (UQ) methods. You can also find the example in this Jupyter Notebook .","title":"Get Started"},{"location":"started/#preparation","text":"Currently, we do not provide Python wheels for installation as we aim to expose as many implementation details as possible. You are recommended to directly use or modify the source code to conduct your experiments. To do that, you can first fork the project and clone it to local with git clone . git clone https://github.com/<your GitHub username>/MUBen.git Or, you can directly clone this repository with git or GitHub CLI suppose you do not intend to do change tracking. # clone with git git clone https://github.com/Yinghao-Li/MUBen.git # or, you can clone with GitHub CLI gh repo clone Yinghao-Li/MUBen The following operations assume that you are already in the project root directory MUBen/ .","title":"Preparation"},{"location":"started/#requirements","text":"Our code is developed with Python 3.10 . Notice that it may not work with Python < 3.9 . It is recommended to create a new conda environment for this project. conda create -n muben python=3.10 The required packages are listed in requirements.txt . It is recommended to install these dependencies with pip install as conda install may sometimes encounter dependency resolution issue. conda activate muben pip install -r requirements.txt","title":"Requirements"},{"location":"started/#docker","text":"Alternatively, you can run this project in a docker container. You can build your image through docker build -t muben ./docker and run your container in the terminal with docker run --gpus all -it --rm muben","title":"Docker"},{"location":"started/#external-dependencies","text":"The backbone models GROVER and Uni-Mol require loading pre-trained model checkpoints. The GROVER-base checkpoint is available at GROVER's project repo or can be directly downloaded through this link . Unzip the downloaded .tar.gz file to get the .pt checkpoint. The Uni-Mol checkpoint is available at Uni-Mol's project repo or can be directly downloaded through this link . By default, the code will look for the models at locations ./models/grover_base.pt and ./models/unimol_base.pt , respectively. You need to specify the --checkpoint_path argument if you prefer other locations and checkpoint names.","title":"External dependencies"},{"location":"started/#a-simple-example","text":"In this demonstration, we'll guide you through foundational training and testing of MUBen using the BBBP dataset as a minimal example. We've chosen the DNN as our backbone model because it is both efficient and offers satisfactory performance. For uncertainty quantification (UQ), we'll evaluate both the Deterministic method (referred to as \"none\" within MUBen) and Temperature Scaling. While the procedures for other backbone models, UQ methods, or datasets are largely similar, you can explore specific variations by referring to API documentation.","title":"A simple example"},{"location":"started/#importing-packages","text":"The first step is to import all the necessary packages from the MUBen source code that defines the datasets, backbone models, UQ methods, and trainers. import logging import wandb from transformers import set_seed from muben.utils.selectors import configure_selector, dataset_selector, model_selector from muben.train import Trainer from muben.utils.io import set_logging # initialize logger logger = logging.getLogger(__name__)","title":"Importing packages"},{"location":"started/#deterministic-method-training","text":"We first train the DNN model Deterministically. That is, we do not apply any UQ method to the model. Instead, we directly use its post-output-activation probabilities as its estimated reliability to its prediction. Here we pass necessary hyper-parameters to the configuration to control the training process. # Set up the logging format and random seed. # We do not use wandb for this demo, so we set its mode to \"disabled\". set_logging() set_seed(42) wandb.init(mode=\"disabled\",) # Select the classes based on the descriptor type. # DNN uses RDKit features, so we set the descriptor type to RDKit and select configuration, dataset, # and model classes according to it. descriptor_type = \"RDKit\" config_class = configure_selector(descriptor_type) dataset_class = dataset_selector(descriptor_type) model_class = model_selector(descriptor_type) # Specify the configuration of the experiment. # Notice that although we directly edit the config object here, a more appropriate way of doing this is # passing arguments through the shell or json scripts when we are running the experiments through the terminal. config = config_class() config.model_name = \"DNN\" config.feature_type = \"rdkit\" config.data_folder = \"../data/files/\" config.dataset_name = \"bbbp\" config.result_folder = \"../output-demo/\" config.uncertainty_method = \"none\" # here \"none\" refers to \"Deterministic\" config.retrain_model = True # We only train the model for a few epochs for the demo. config.n_epochs = 50 # activate training timer config.time_training = True # Post initialization of the arguments. config.__post_init__() # Load dataset metadata, validate the arguments, and log the configuration. _ = config.get_meta().validate().log() The configuration details are printed out in your terminal by calling config.log() . Similar to configuration, we automatically infer the dataset and collate function classes according to the descriptor type we set above. Then, we initialize the training, validation, and test datasets. # Load and process the training, validation and test datasets dataset_class, collator_class = dataset_selector(descriptor_type) training_dataset = dataset_class().prepare(config=config, partition=\"train\") valid_dataset = dataset_class().prepare(config=config, partition=\"valid\") test_dataset = dataset_class().prepare(config=config, partition=\"test\") Afterward, we can initialize the trainer and model with our configuration. model_selector automatically detects the model type according to the descriptor. In this case, DNN is the selected model. Then, the trainer initializes the model with arguments defined in the configuration. # Inintialized the trainer with the configuration and datasets # Inintialized the trainer with the configuration and datasets trainer = Trainer( config=config, model_class=model_selector(descriptor_type), training_dataset=training_dataset, valid_dataset=valid_dataset, test_dataset=test_dataset, collate_fn=collator_class(config), ).initialize(config=config) Once the trainer is initialized, we can call trainer.run() to do the training. # Run the training, validation and test process. # The model checkpoint and predicted results will be automatically saved in the specified output folder. trainer.run()","title":"Deterministic method -- training"},{"location":"started/#temperature-scaling-training","text":"Training the DNN model with Temperature Scaling is basically identical to training with the Deterministic method. The only difference is that we need to define the uncertainty_method in config as \"TemperatureScaling\" instead of \"none\" . wandb.init(mode=\"disabled\",) # Change some configuration items. config.uncertainty_method = \"TemperatureScaling\" config.retrain_model = False config.n_ts_epochs = 10 # number of epochs for training the temperature scaling layer. config.__post_init__() _ = config.validate().log() # Re-inintialized the trainer with the updated configuration. # The datasets are not changed. trainer = Trainer( config=config, model_class=model_selector(descriptor_type), training_dataset=training_dataset, valid_dataset=valid_dataset, test_dataset=test_dataset, collate_fn=collator_class(config), ).initialize(config=config) # Run the training, validation and test process. # The trainer will load the model checkpoint from the Deterministic run and # continue training the temperature scaling layer. # Notice that not all UQ methods support continued training. For example, BBP requires training from scratch. trainer.run()","title":"Temperature Scaling -- training"},{"location":"started/#evaluation","text":"Here, we provide a simplified version of metric calculation. Please check <project root>/assist/result_get_metrics.py for the full function. import os.path as osp import pandas as pd from muben.utils.metrics import classification_metrics from muben.utils.io import load_results # Define the path to the predicted results. \"det\" stands for \"Deterministic\"; \"ts\" stands for \"Temperature Scaling\". det_result = osp.join( config.result_folder, config.dataset_name, f\"{config.model_name}-{config.feature_type}\", \"none\", f\"seed-{config.seed}\", \"preds\", \"0.pt\" ) ts_result = osp.join( config.result_folder, config.dataset_name, f\"{config.model_name}-{config.feature_type}\", \"TemperatureScaling\", f\"seed-{config.seed}\", \"preds\", \"0.pt\" ) # Load the predicted results. det_preds, _, lbs, masks = load_results([det_result]) ts_preds, _, _, _ = load_results([ts_result]) # Calculate the metrics. det_metrics = classification_metrics(det_preds, lbs, masks) ts_metrics = classification_metrics(ts_preds, lbs, masks) det_metrics = {k: v['macro-avg'] for k, v in det_metrics.items()} ts_metrics = {k: v['macro-avg'] for k, v in ts_metrics.items()} # Present the results in a dataframe. det_metrics_df = pd.DataFrame({\"Deterministic\": det_metrics, \"TemperatureScaling\": ts_metrics}) print(det_metrics_df.T) The result will be presented as a table the columns being metrics and rows being the UQ method.","title":"Evaluation"}]}