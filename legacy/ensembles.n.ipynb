{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import logging\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchmetrics.functional.classification import binary_calibration_error\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from scipy.stats import norm as gaussian\n",
    "\n",
    "from muben.utils.io import set_logging, init_dir, load_results\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_seeded_results(results_for_seeds: list[dict[str, float]]):\n",
    "    assert results_for_seeds\n",
    "\n",
    "    results_aggr = {\n",
    "        metric: [r.get(metric, np.NaN) for r in results_for_seeds]\n",
    "        for metric in list(results_for_seeds[0].keys())\n",
    "    }\n",
    "    for k in results_aggr:\n",
    "        mean = np.nanmean(results_aggr[k])\n",
    "        std = np.nanstd(results_aggr[k])\n",
    "        results_aggr[k] = {\"mean\": mean, \"std\": std}\n",
    "\n",
    "    return results_aggr\n",
    "\n",
    "\n",
    "def save_results(results, result_dir, model_name, dataset_name):\n",
    "    if not results:\n",
    "        logger.warning(\"Result dict is empty. No results is saved!\")\n",
    "        return None\n",
    "\n",
    "    uncertainty_names = list(results.keys())\n",
    "    metrics = list(list(results.values())[0].keys())\n",
    "    columns_headers = (\n",
    "        [\"method\"]\n",
    "        + [f\"{metric}-mean\" for metric in metrics]\n",
    "        + [f\"{metric}-std\" for metric in metrics]\n",
    "    )\n",
    "    columns = {k: list() for k in columns_headers}\n",
    "    columns[\"method\"] = [f\"{model_name}-{un}\" for un in uncertainty_names]\n",
    "    for uncertainty in uncertainty_names:\n",
    "        metric_dict = results[uncertainty]\n",
    "        for metric in metrics:\n",
    "            value = metric_dict.get(metric, None)\n",
    "            if value:\n",
    "                columns[f\"{metric}-mean\"].append(value[\"mean\"])\n",
    "                columns[f\"{metric}-std\"].append(value[\"std\"])\n",
    "            else:\n",
    "                columns[f\"{metric}-mean\"].append(np.NaN)\n",
    "                columns[f\"{metric}-std\"].append(np.NaN)\n",
    "\n",
    "    df = pd.DataFrame(columns)\n",
    "    init_dir(op.join(result_dir, \"RESULTS\", \"scores\"), clear_original_content=False)\n",
    "    df.to_csv(\n",
    "        op.join(result_dir, \"RESULTS\", \"scores\", f\"{model_name}-{dataset_name}.csv\")\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "def classification_metrics(preds, lbs, masks):\n",
    "    result_metrics_dict = dict()\n",
    "\n",
    "    roc_auc_list = list()\n",
    "    prc_auc_list = list()\n",
    "    ece_list = list()\n",
    "    mce_list = list()\n",
    "    nll_list = list()\n",
    "    brier_list = list()\n",
    "\n",
    "    roc_auc_valid_flag = True\n",
    "    prc_auc_valid_flag = True\n",
    "    ece_valid_flag = True\n",
    "    mce_valid_flag = True\n",
    "    nll_valid_flag = True\n",
    "    brier_valid_flag = True\n",
    "\n",
    "    for i in range(lbs.shape[-1]):\n",
    "        lbs_ = lbs[:, i][masks[:, i].astype(bool)]\n",
    "        preds_ = preds[:, i][masks[:, i].astype(bool)]\n",
    "\n",
    "        if len(lbs_) < 1:\n",
    "            continue\n",
    "        if (lbs_ < 0).any():\n",
    "            raise ValueError(\"Invalid label value encountered!\")\n",
    "        if (lbs_ == 0).all() or (\n",
    "            lbs_ == 1\n",
    "        ).all():  # skip tasks with only one label type, as Uni-Mol did.\n",
    "            continue\n",
    "\n",
    "        # --- roc-auc ---\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(lbs_, preds_)\n",
    "            roc_auc_list.append(roc_auc)\n",
    "        except:\n",
    "            roc_auc_valid_flag = False\n",
    "\n",
    "        # --- prc-auc ---\n",
    "        try:\n",
    "            p, r, _ = precision_recall_curve(lbs_, preds_)\n",
    "            prc_auc = auc(r, p)\n",
    "            prc_auc_list.append(prc_auc)\n",
    "        except:\n",
    "            prc_auc_valid_flag = False\n",
    "\n",
    "        # --- ece ---\n",
    "        try:\n",
    "            ece = binary_calibration_error(\n",
    "                torch.from_numpy(preds_), torch.from_numpy(lbs_)\n",
    "            ).item()\n",
    "            ece_list.append(ece)\n",
    "        except:\n",
    "            ece_valid_flag = False\n",
    "\n",
    "        # --- mce ---\n",
    "        try:\n",
    "            mce = binary_calibration_error(\n",
    "                torch.from_numpy(preds_), torch.from_numpy(lbs_), norm=\"max\"\n",
    "            ).item()\n",
    "            mce_list.append(mce)\n",
    "        except:\n",
    "            mce_valid_flag = False\n",
    "\n",
    "        # --- nll ---\n",
    "        try:\n",
    "            nll = F.binary_cross_entropy(\n",
    "                input=torch.from_numpy(preds_),\n",
    "                target=torch.from_numpy(lbs_).to(torch.float),\n",
    "                reduction=\"mean\",\n",
    "            ).item()\n",
    "            nll_list.append(nll)\n",
    "        except:\n",
    "            nll_valid_flag = False\n",
    "\n",
    "        # --- brier ---\n",
    "        try:\n",
    "            brier = brier_score_loss(lbs_, preds_)\n",
    "            brier_list.append(brier)\n",
    "        except:\n",
    "            brier_valid_flag = False\n",
    "\n",
    "    if roc_auc_valid_flag:\n",
    "        roc_auc_avg = np.mean(roc_auc_list)\n",
    "        result_metrics_dict[\"roc-auc\"] = {\"all\": roc_auc_list, \"macro-avg\": roc_auc_avg}\n",
    "\n",
    "    if prc_auc_valid_flag:\n",
    "        prc_auc_avg = np.mean(prc_auc_list)\n",
    "        result_metrics_dict[\"prc-auc\"] = {\"all\": prc_auc_list, \"macro-avg\": prc_auc_avg}\n",
    "\n",
    "    if ece_valid_flag:\n",
    "        ece_avg = np.mean(ece_list)\n",
    "        result_metrics_dict[\"ece\"] = {\"all\": ece_list, \"macro-avg\": ece_avg}\n",
    "\n",
    "    if mce_valid_flag:\n",
    "        mce_avg = np.mean(mce_list)\n",
    "        result_metrics_dict[\"mce\"] = {\"all\": mce_list, \"macro-avg\": mce_avg}\n",
    "\n",
    "    if nll_valid_flag:\n",
    "        nll_avg = np.mean(nll_list)\n",
    "        result_metrics_dict[\"nll\"] = {\"all\": nll_list, \"macro-avg\": nll_avg}\n",
    "\n",
    "    if brier_valid_flag:\n",
    "        brier_avg = np.mean(brier_list)\n",
    "        result_metrics_dict[\"brier\"] = {\"all\": brier_list, \"macro-avg\": brier_avg}\n",
    "\n",
    "    return result_metrics_dict\n",
    "\n",
    "\n",
    "def regression_metrics(preds, variances, lbs, masks):\n",
    "    if len(preds.shape) == 1:\n",
    "        preds = preds[:, np.newaxis]\n",
    "\n",
    "    if len(variances.shape) == 1:\n",
    "        variances = variances[:, np.newaxis]\n",
    "\n",
    "    # --- rmse ---\n",
    "    result_metrics_dict = dict()\n",
    "\n",
    "    rmse_list = list()\n",
    "    mae_list = list()\n",
    "    nll_list = list()\n",
    "    ce_list = list()\n",
    "\n",
    "    for i in range(lbs.shape[-1]):\n",
    "        lbs_ = lbs[:, i][masks[:, i].astype(bool)]\n",
    "        preds_ = preds[:, i][masks[:, i].astype(bool)]\n",
    "        vars_ = variances[:, i][masks[:, i].astype(bool)]\n",
    "\n",
    "        # --- rmse ---\n",
    "        rmse = mean_squared_error(lbs_, preds_, squared=False)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "        # --- mae ---\n",
    "        mae = mean_absolute_error(lbs_, preds_)\n",
    "        mae_list.append(mae)\n",
    "\n",
    "        # --- Gaussian NLL ---\n",
    "        nll = F.gaussian_nll_loss(\n",
    "            torch.from_numpy(preds_), torch.from_numpy(lbs_), torch.from_numpy(vars_)\n",
    "        ).item()\n",
    "        nll_list.append(nll)\n",
    "\n",
    "        # --- calibration error ---\n",
    "        ce = regression_calibration_error(lbs_, preds_, vars_)\n",
    "        ce_list.append(ce)\n",
    "\n",
    "    rmse_avg = np.mean(rmse_list)\n",
    "    result_metrics_dict[\"rmse\"] = {\"all\": rmse_list, \"macro-avg\": rmse_avg}\n",
    "\n",
    "    mae_avg = np.mean(mae_list)\n",
    "    result_metrics_dict[\"mae\"] = {\"all\": mae_list, \"macro-avg\": mae_avg}\n",
    "\n",
    "    nll_avg = np.mean(nll_list)\n",
    "    result_metrics_dict[\"nll\"] = {\"all\": nll_list, \"macro-avg\": nll_avg}\n",
    "\n",
    "    ce_avg = np.mean(ce_list)\n",
    "    result_metrics_dict[\"ce\"] = {\"all\": ce_list, \"macro-avg\": ce_avg}\n",
    "\n",
    "    return result_metrics_dict\n",
    "\n",
    "\n",
    "def regression_calibration_error(lbs, preds, variances, n_bins=20):\n",
    "    sigma = np.sqrt(variances)\n",
    "    phi_lbs = gaussian.cdf(lbs, loc=preds.reshape(-1, 1), scale=sigma.reshape(-1, 1))\n",
    "\n",
    "    expected_confidence = np.linspace(0, 1, n_bins + 1)[1:-1]\n",
    "    observed_confidence = np.zeros_like(expected_confidence)\n",
    "\n",
    "    for i in range(0, len(expected_confidence)):\n",
    "        observed_confidence[i] = np.mean(phi_lbs <= expected_confidence[i])\n",
    "\n",
    "    calibration_error = np.mean(\n",
    "        (expected_confidence.ravel() - observed_confidence.ravel()) ** 2\n",
    "    )\n",
    "\n",
    "    return calibration_error\n",
    "\n",
    "def load_results_no_merging(result_paths: list[str]):\n",
    "    \"\"\"\n",
    "    Load muben prediction results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_paths : list[str]\n",
    "        paths to the result files\n",
    "    \"\"\"\n",
    "    lbs = masks = np.nan\n",
    "    preds_list = list()\n",
    "    variances_list = list()\n",
    "\n",
    "    for test_result_path in result_paths:\n",
    "        results = torch.load(test_result_path)\n",
    "\n",
    "        if lbs is not np.nan:\n",
    "            assert (lbs == results[\"lbs\"]).all()\n",
    "        else:\n",
    "            lbs: np.ndarray = results[\"lbs\"]\n",
    "\n",
    "        if masks is not np.nan:\n",
    "            assert (masks == results[\"masks\"]).all()\n",
    "        else:\n",
    "            masks: np.ndarray = results[\"masks\"]\n",
    "\n",
    "        if results.get(\"version\", 1) == 1:\n",
    "            preds_list.append(results[\"preds\"][\"preds\"])\n",
    "            try:\n",
    "                variances_list.append(results[\"preds\"][\"vars\"])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        elif results.get(\"version\", 1) == 2:\n",
    "            preds_list.append(results[\"preds\"])\n",
    "            try:\n",
    "                variances_list.append(results[\"vars\"])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Undefined result version: {results.get('version', 1)}\"\n",
    "            )\n",
    "\n",
    "    # aggregate mean and variance\n",
    "    preds = np.stack(preds_list)\n",
    "    if (\n",
    "        variances_list and not (np.asarray(variances_list) == None).any()\n",
    "    ):  # regression\n",
    "        # variances = np.mean(np.stack(preds_list) ** 2 + np.stack(variances_list), axis=0) - preds ** 2\n",
    "        variances = np.stack(variances_list)\n",
    "    else:\n",
    "        variances = None\n",
    "\n",
    "    return preds, variances, lbs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"lipo\"\n",
    "# dataset = \"tox21\"\n",
    "uncertainty = \"DeepEnsembles\"\n",
    "# model = \"DNN-rdkit\"\n",
    "model = \"Uni-Mol\"\n",
    "result_folder = f\"../output/{dataset}/{model}/{uncertainty}\"\n",
    "seeds = list(range(10))\n",
    "\n",
    "results = dict()\n",
    "for idx in range(len(seeds)):\n",
    "    seed_ids = seeds[:idx+1]\n",
    "    test_result_paths = [\n",
    "        op.join(result_folder, f\"seed-{sid}\", \"preds\", \"0.pt\") for sid in seed_ids\n",
    "    ]\n",
    "\n",
    "    preds, variances, lbs, masks = load_results(test_result_paths)\n",
    "\n",
    "    if variances is not None:  # regression\n",
    "        metrics = regression_metrics(preds, variances, lbs, masks)\n",
    "    else:  # classification\n",
    "        metrics = classification_metrics(preds, lbs, masks)\n",
    "\n",
    "    result_dict = {k: v[\"macro-avg\"] for k, v in metrics.items()}\n",
    "    results_aggr = {\n",
    "        k: v for k, v in result_dict.items()\n",
    "    }\n",
    "    results[idx+1] = results_aggr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(results)\n",
    "df = df.round(3)\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"lipo\"\n",
    "dataset = \"tox21\"\n",
    "uncertainty = \"DeepEnsembles\"\n",
    "# model = \"DNN-rdkit\"\n",
    "model = \"Uni-Mol\"\n",
    "result_folder = f\"../output/{dataset}/{model}/{uncertainty}\"\n",
    "seeds = list(range(10))\n",
    "\n",
    "mean_variances_dict = dict()\n",
    "var_variances_dict = dict()\n",
    "for idx in range(len(seeds)):\n",
    "    seed_ids = seeds[:idx+1]\n",
    "    test_result_paths = [\n",
    "        op.join(result_folder, f\"seed-{sid}\", \"preds\", \"0.pt\") for sid in seed_ids\n",
    "    ]\n",
    "\n",
    "    means, variances, lbs, masks = load_results_no_merging(test_result_paths)\n",
    "    mean_variances_dict[idx+1] = np.var(means, axis=0).mean()\n",
    "    if variances is not None:\n",
    "        var_variances_dict[idx+1] = np.var(variances, axis=0).mean()\n",
    "\n",
    "results[f\"{model}-{dataset}-mean\"] = mean_variances_dict\n",
    "if variances is not None:\n",
    "    results[f\"{model}-{dataset}-var\"] = var_variances_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   DNN-rdkit-lipo-mean |   DNN-rdkit-lipo-var |   Uni-Mol-lipo-mean |   Uni-Mol-lipo-var |   DNN-rdkit-tox21-mean |   Uni-Mol-tox21-mean |\n",
      "|---:|----------------------:|---------------------:|--------------------:|-------------------:|-----------------------:|---------------------:|\n",
      "|  1 |               0       |              0       |             0       |            0       |                0       |              0       |\n",
      "|  2 |               0.04634 |              0.00051 |             0.01311 |            0.00027 |                0.00287 |              0.00224 |\n",
      "|  3 |               0.06198 |              0.0008  |             0.0163  |            0.00031 |                0.00345 |              0.00279 |\n",
      "|  4 |               0.06801 |              0.00124 |             0.01741 |            0.00026 |                0.00357 |              0.00374 |\n",
      "|  5 |               0.06867 |              0.00255 |             0.02351 |            0.00284 |                0.00356 |              0.00421 |\n",
      "|  6 |               0.07439 |              0.00352 |             0.02559 |            0.00281 |                0.00401 |              0.00521 |\n",
      "|  7 |               0.07661 |              0.00319 |             0.02545 |            0.00244 |                0.00394 |              0.00533 |\n",
      "|  8 |               0.07548 |              0.00472 |             0.02526 |            0.00218 |                0.004   |              0.00542 |\n",
      "|  9 |               0.07658 |              0.00638 |             0.0253  |            0.00214 |                0.00393 |              0.00565 |\n",
      "| 10 |               0.07713 |              0.00628 |             0.02555 |            0.00194 |                0.00384 |              0.00561 |\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(results)\n",
    "df = df.round(5)\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
