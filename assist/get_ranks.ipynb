{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:52:16.449762Z",
     "start_time": "2023-06-13T15:52:16.023873Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mubench.utils.macro import CLASSIFICATION_DATASET, REGRESSION_DATASET\n",
    "from mubench.utils.io import init_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "CLASSIFICATION_METRICS = ['roc-auc', 'prc-auc', 'ece', 'mce', 'nll', 'brier']\n",
    "REGRESSION_METRICS = ['rmse', 'mae', 'nll', 'ce']\n",
    "LARGER_BETTER_LOOKUP = {\n",
    "    'roc-auc': True,\n",
    "    'prc-auc': True,\n",
    "    'ece': False,\n",
    "    'mce': False,\n",
    "    'nll': False,\n",
    "    'brier': False,\n",
    "    'rmse': False,\n",
    "    'mae': False,\n",
    "    'ce': False\n",
    "}\n",
    "MODEL_NAMES = [\n",
    "    \"DNN-rdkit\",\n",
    "    \"ChemBERTa\",\n",
    "    \"GROVER\",\n",
    "    \"Uni-Mol\"\n",
    "]\n",
    "CLASSIFICATION_UNCERTAINTY = [\n",
    "    'none',\n",
    "    'TemperatureScaling',\n",
    "    'FocalLoss',\n",
    "    'MCDropout',\n",
    "    'SWAG',\n",
    "    'BBP',\n",
    "    'SGLD',\n",
    "    'DeepEnsembles'\n",
    "]\n",
    "REGRESSION_UNCERTAINTY = [\n",
    "    'none',\n",
    "    'MCDropout',\n",
    "    'SWAG',\n",
    "    'BBP',\n",
    "    'SGLD',\n",
    "    'DeepEnsembles'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:52:28.784606Z",
     "start_time": "2023-06-13T15:52:28.781727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_ranks(values, smaller_is_better=True):\n",
    "    arr = np.array(values)\n",
    "    if smaller_is_better:\n",
    "        order = arr.argsort()\n",
    "    else:\n",
    "        order = arr.argsort()[::-1]\n",
    "    rnks = order.argsort()\n",
    "    return rnks + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:52:45.486938Z",
     "start_time": "2023-06-13T15:52:45.484389Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "result_path = '../output/RESULTS/'\n",
    "score_path = op.join(result_path, 'scores')\n",
    "result_files = list(glob.glob(op.join(score_path, '*.csv')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:52:47.733974Z",
     "start_time": "2023-06-13T15:52:47.728283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "cla_dt_md_unc_mtr = dict()\n",
    "reg_dt_md_unc_mtr = dict()\n",
    "for result_file in result_files:\n",
    "\n",
    "    file_name = op.basename(result_file)\n",
    "    model_name = '-'.join(file_name.split('-')[:-1])\n",
    "    dataset_name = file_name.split('-')[-1].split('.')[0]\n",
    "\n",
    "    if dataset_name in CLASSIFICATION_DATASET:\n",
    "        dataset_model_uncertainty_metric = cla_dt_md_unc_mtr\n",
    "    elif dataset_name in REGRESSION_DATASET:\n",
    "        dataset_model_uncertainty_metric = reg_dt_md_unc_mtr\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    if dataset_name not in dataset_model_uncertainty_metric.keys():\n",
    "        dataset_model_uncertainty_metric[dataset_name] = dict()\n",
    "\n",
    "    df = pd.read_csv(result_file)\n",
    "\n",
    "    uncertainty_methods = df.method\n",
    "    uncertainty_methods = [um.split('-')[-1] for um in uncertainty_methods]\n",
    "\n",
    "    metric_means = [k for k in df.keys() if k.endswith('-mean')]\n",
    "    metric_names = [m[:-5] for m in metric_means]\n",
    "    column_headers = ['method'] + metric_means\n",
    "\n",
    "    uncertainty_metric = dict()\n",
    "    for item in df[column_headers].to_numpy():\n",
    "        uncertainty_method = item[0].split('-')[-1]\n",
    "        metric_values = item[1:]\n",
    "        assert len(metric_values) == len(metric_names)\n",
    "\n",
    "        uncertainty_metric[uncertainty_method] = {n: v for n, v in zip(metric_names, metric_values)}\n",
    "\n",
    "    dataset_model_uncertainty_metric[dataset_name][model_name] = uncertainty_metric\n",
    "\n",
    "cla_fl_dt_md_unc_mtr = pd.json_normalize(cla_dt_md_unc_mtr, sep='_').to_dict()\n",
    "reg_fl_dt_md_unc_mtr = pd.json_normalize(reg_dt_md_unc_mtr, sep='_').to_dict()\n",
    "\n",
    "cla_dt_fl_md_unc_mtr = {dt: pd.json_normalize(cla_md_unc_mtr, sep='_').to_dict() for dt, cla_md_unc_mtr in cla_dt_md_unc_mtr.items()}\n",
    "reg_dt_fl_md_unc_mtr = {dt: pd.json_normalize(reg_md_unc_mtr, sep='_').to_dict() for dt, reg_md_unc_mtr in reg_dt_md_unc_mtr.items()}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:53:04.119601Z",
     "start_time": "2023-06-13T15:53:03.983023Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "classification_datasets = list(cla_dt_fl_md_unc_mtr.keys())\n",
    "regression_datasets = list(reg_dt_fl_md_unc_mtr.keys())\n",
    "\n",
    "cla_metric_ranks = {dt: {mtr: dict() for mtr in CLASSIFICATION_METRICS} for dt in classification_datasets}\n",
    "reg_metric_ranks = {dt: {mtr: dict() for mtr in REGRESSION_METRICS} for dt in regression_datasets}\n",
    "\n",
    "cla_metric_ranks_mean = {mtr: dict() for mtr in CLASSIFICATION_METRICS}\n",
    "reg_metric_ranks_mean = {mtr: dict() for mtr in REGRESSION_METRICS}\n",
    "\n",
    "for dt, mtr_vals in cla_metric_ranks.items():\n",
    "    for mtr in mtr_vals:\n",
    "        md_unc_scores = {'_'.join(k.split('_')[1:-1]): val[0] for k, val in cla_fl_dt_md_unc_mtr.items() if k.startswith(dt) and k.endswith(mtr)}\n",
    "        ranks = get_ranks(list(md_unc_scores.values()), smaller_is_better=not LARGER_BETTER_LOOKUP[mtr])\n",
    "        md_unc_ranks = {k: r for k, r in zip(md_unc_scores, ranks)}\n",
    "        mtr_vals[mtr] = md_unc_ranks\n",
    "\n",
    "        dict1 = cla_metric_ranks_mean[mtr]\n",
    "        dict2 = md_unc_ranks\n",
    "        cla_metric_ranks_mean[mtr] = {i: dict1.get(i, 0) + dict2.get(i, 0) / len(classification_datasets)\n",
    "                                      for i in md_unc_ranks.keys()}\n",
    "\n",
    "for dt, mtr_vals in reg_metric_ranks.items():\n",
    "    for mtr in mtr_vals:\n",
    "        md_unc_scores = {'_'.join(k.split('_')[1:-1]): val[0] for k, val in reg_fl_dt_md_unc_mtr.items() if k.startswith(dt) and k.endswith(mtr)}\n",
    "        ranks = get_ranks(list(md_unc_scores.values()), smaller_is_better=not LARGER_BETTER_LOOKUP[mtr])\n",
    "        md_unc_ranks = {k: r for k, r in zip(md_unc_scores, ranks)}\n",
    "        mtr_vals[mtr] = md_unc_ranks\n",
    "\n",
    "        dict1 = reg_metric_ranks_mean[mtr]\n",
    "        dict2 = md_unc_ranks\n",
    "        reg_metric_ranks_mean[mtr] = {i: dict1.get(i, 0) + dict2.get(i, 0) / len(regression_datasets)\n",
    "                                      for i in md_unc_ranks.keys()}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:53:07.449881Z",
     "start_time": "2023-06-13T15:53:07.443054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "output_dir = op.join(result_path, 'ranks')\n",
    "init_dir(output_dir)\n",
    "\n",
    "for dataset in classification_datasets:\n",
    "    dataset_metric_ranks = cla_metric_ranks[dataset]\n",
    "    df = pd.DataFrame(dataset_metric_ranks)\n",
    "    df.to_csv(op.join(output_dir, f'{dataset}.csv'))\n",
    "\n",
    "for dataset in regression_datasets:\n",
    "    dataset_metric_ranks = reg_metric_ranks[dataset]\n",
    "    df = pd.DataFrame(dataset_metric_ranks)\n",
    "    df.to_csv(op.join(output_dir, f'{dataset}.csv'))\n",
    "\n",
    "df = pd.DataFrame(cla_metric_ranks_mean)\n",
    "df.to_csv(op.join(output_dir, 'mean_classification.csv'))\n",
    "\n",
    "df = pd.DataFrame(reg_metric_ranks_mean)\n",
    "df.to_csv(op.join(output_dir, 'mean_regression.csv'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T15:53:13.439904Z",
     "start_time": "2023-06-13T15:53:13.322396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "cla_metric_rrs = {dt: {mtr: dict() for mtr in CLASSIFICATION_METRICS} for dt in classification_datasets}\n",
    "reg_metric_rrs = {dt: {mtr: dict() for mtr in REGRESSION_METRICS} for dt in regression_datasets}\n",
    "\n",
    "cla_metric_rrs_mean = {mtr: dict() for mtr in CLASSIFICATION_METRICS}\n",
    "reg_metric_rrs_mean = {mtr: dict() for mtr in REGRESSION_METRICS}\n",
    "\n",
    "for dt, mtr_vals in cla_metric_rrs.items():\n",
    "    for mtr in mtr_vals:\n",
    "        md_unc_scores = {'_'.join(k.split('_')[1:-1]): val[0] for k, val in cla_fl_dt_md_unc_mtr.items() if k.startswith(dt) and k.endswith(mtr)}\n",
    "        rrs = get_ranks(list(md_unc_scores.values()), smaller_is_better=not LARGER_BETTER_LOOKUP[mtr])\n",
    "        md_unc_rrs = {k: 1/r for k, r in zip(md_unc_scores, rrs)}\n",
    "        mtr_vals[mtr] = md_unc_rrs\n",
    "\n",
    "        dict1 = cla_metric_rrs_mean[mtr]\n",
    "        dict2 = md_unc_rrs\n",
    "        cla_metric_rrs_mean[mtr] = {i: dict1.get(i, 0) + dict2.get(i, 0) / len(classification_datasets) for i in md_unc_rrs.keys()}\n",
    "\n",
    "for dt, mtr_vals in reg_metric_rrs.items():\n",
    "    for mtr in mtr_vals:\n",
    "        md_unc_scores = {'_'.join(k.split('_')[1:-1]): val[0] for k, val in reg_fl_dt_md_unc_mtr.items() if k.startswith(dt) and k.endswith(mtr)}\n",
    "        rrs = get_ranks(list(md_unc_scores.values()), smaller_is_better=not LARGER_BETTER_LOOKUP[mtr])\n",
    "        md_unc_rrs = {k: 1/r for k, r in zip(md_unc_scores, rrs)}\n",
    "        mtr_vals[mtr] = md_unc_rrs\n",
    "\n",
    "        dict1 = reg_metric_rrs_mean[mtr]\n",
    "        dict2 = md_unc_rrs\n",
    "        reg_metric_rrs_mean[mtr] = {i: dict1.get(i, 0) + dict2.get(i, 0) / len(regression_datasets) for i in md_unc_rrs.keys()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T19:14:13.818722Z",
     "start_time": "2023-06-06T19:14:13.806065Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "output_dir = op.join(result_path, 'mrrs')\n",
    "init_dir(output_dir)\n",
    "\n",
    "for dataset in classification_datasets:\n",
    "    dataset_metric_mrrs = cla_metric_rrs[dataset]\n",
    "    df = pd.DataFrame(dataset_metric_mrrs)\n",
    "    df.to_csv(op.join(output_dir, f'{dataset}-mrr.csv'))\n",
    "\n",
    "for dataset in regression_datasets:\n",
    "    dataset_metric_mrrs = reg_metric_rrs[dataset]\n",
    "    df = pd.DataFrame(dataset_metric_mrrs)\n",
    "    df.to_csv(op.join(output_dir, f'{dataset}-mrr.csv'))\n",
    "\n",
    "df = pd.DataFrame(cla_metric_rrs_mean)\n",
    "df.to_csv(op.join(output_dir, 'mrr_classification.csv'))\n",
    "\n",
    "df = pd.DataFrame(reg_metric_rrs_mean)\n",
    "df.to_csv(op.join(output_dir, 'mrr_regression.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T19:14:13.827148Z",
     "start_time": "2023-06-06T19:14:13.817218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "cla_dt_md_mtr_unc_rnks = dict()\n",
    "unc_methods = None\n",
    "for dt, md_unc_mtr_values in cla_dt_md_unc_mtr.items():\n",
    "\n",
    "    if dt not in cla_dt_md_mtr_unc_rnks:\n",
    "        cla_dt_md_mtr_unc_rnks[dt] = dict()\n",
    "\n",
    "    for md, unc_mtr_values in md_unc_mtr_values.items():\n",
    "\n",
    "        if unc_methods is None:\n",
    "            unc_methods = list(unc_mtr_values.keys())\n",
    "\n",
    "        if md not in cla_dt_md_mtr_unc_rnks[dt]:\n",
    "            cla_dt_md_mtr_unc_rnks[dt][md] = dict()\n",
    "\n",
    "        for unc, mtr_values in unc_mtr_values.items():\n",
    "\n",
    "            for mtr in CLASSIFICATION_METRICS:\n",
    "                if mtr not in cla_dt_md_mtr_unc_rnks[dt][md]:\n",
    "                    cla_dt_md_mtr_unc_rnks[dt][md][mtr] = dict()\n",
    "\n",
    "                cla_dt_md_mtr_unc_rnks[dt][md][mtr][unc] = mtr_values[mtr]\n",
    "\n",
    "        for mtr in CLASSIFICATION_METRICS:\n",
    "            ranks = get_ranks(list(cla_dt_md_mtr_unc_rnks[dt][md][mtr].values()), smaller_is_better=not LARGER_BETTER_LOOKUP[mtr])\n",
    "            unc_ranks = {unc: r for unc, r in zip(cla_dt_md_mtr_unc_rnks[dt][md][mtr], ranks)}\n",
    "            cla_dt_md_mtr_unc_rnks[dt][md][mtr] = unc_ranks\n",
    "\n",
    "cla_avg_md_mtr_unc_rnks = dict()\n",
    "for md in MODEL_NAMES:\n",
    "    for unc in CLASSIFICATION_UNCERTAINTY:\n",
    "        md_unc = f\"{md}-{unc}\"\n",
    "        if md_unc not in cla_avg_md_mtr_unc_rnks:\n",
    "            cla_avg_md_mtr_unc_rnks[md_unc] = dict()\n",
    "\n",
    "        for mtr in CLASSIFICATION_METRICS:\n",
    "            avg_rank = np.mean([cla_dt_md_mtr_unc_rnks[dt][md][mtr][unc] for dt in CLASSIFICATION_DATASET])\n",
    "            cla_avg_md_mtr_unc_rnks[md_unc][mtr] = avg_rank\n",
    "\n",
    "\n",
    "reg_dt_md_mtr_unc_rnks = dict()\n",
    "unc_methods = None\n",
    "for dt, md_unc_mtr_values in reg_dt_md_unc_mtr.items():\n",
    "\n",
    "    if dt not in reg_dt_md_mtr_unc_rnks:\n",
    "        reg_dt_md_mtr_unc_rnks[dt] = dict()\n",
    "\n",
    "    for md, unc_mtr_values in md_unc_mtr_values.items():\n",
    "\n",
    "        if unc_methods is None:\n",
    "            unc_methods = list(unc_mtr_values.keys())\n",
    "\n",
    "        if md not in reg_dt_md_mtr_unc_rnks[dt]:\n",
    "            reg_dt_md_mtr_unc_rnks[dt][md] = dict()\n",
    "\n",
    "        for unc, mtr_values in unc_mtr_values.items():\n",
    "\n",
    "            for mtr in REGRESSION_METRICS:\n",
    "                if mtr not in reg_dt_md_mtr_unc_rnks[dt][md]:\n",
    "                    reg_dt_md_mtr_unc_rnks[dt][md][mtr] = dict()\n",
    "\n",
    "                reg_dt_md_mtr_unc_rnks[dt][md][mtr][unc] = mtr_values[mtr]\n",
    "\n",
    "        for mtr in REGRESSION_METRICS:\n",
    "            ranks = get_ranks(list(reg_dt_md_mtr_unc_rnks[dt][md][mtr].values()), smaller_is_better=not LARGER_BETTER_LOOKUP[mtr])\n",
    "            unc_ranks = {unc: r for unc, r in zip(reg_dt_md_mtr_unc_rnks[dt][md][mtr], ranks)}\n",
    "            reg_dt_md_mtr_unc_rnks[dt][md][mtr] = unc_ranks\n",
    "\n",
    "reg_avg_md_mtr_unc_rnks = dict()\n",
    "for md in MODEL_NAMES:\n",
    "    for unc in REGRESSION_UNCERTAINTY:\n",
    "        md_unc = f\"{md}-{unc}\"\n",
    "        if md_unc not in reg_avg_md_mtr_unc_rnks:\n",
    "            reg_avg_md_mtr_unc_rnks[md_unc] = dict()\n",
    "\n",
    "        for mtr in REGRESSION_METRICS:\n",
    "            avg_rank = np.mean([reg_dt_md_mtr_unc_rnks[dt][md][mtr][unc] for dt in REGRESSION_DATASET])\n",
    "            reg_avg_md_mtr_unc_rnks[md_unc][mtr] = avg_rank"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T19:14:13.797157Z",
     "start_time": "2023-06-06T19:14:13.792120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "output_dir = op.join(result_path, 'ranks')\n",
    "init_dir(output_dir, clear_original_content=False)\n",
    "\n",
    "df = pd.DataFrame(cla_avg_md_mtr_unc_rnks)\n",
    "df.to_csv(op.join(output_dir, 'mean_classification.csv'))\n",
    "\n",
    "df = pd.DataFrame(reg_avg_md_mtr_unc_rnks)\n",
    "df.to_csv(op.join(output_dir, 'mean_regression.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T19:14:13.801951Z",
     "start_time": "2023-06-06T19:14:13.799040Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
