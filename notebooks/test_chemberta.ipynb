{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/631 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "126a2bcb95494f4c9f847194f5a408d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/13.7M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1007bd7e1c3c4f98a282ec1a3dbd074d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MLM were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55b8a8d46ec348a9b06448167b760ffc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/6.96k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f8c84ce8bfb4dc69a61527d76286fab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78f9a8719df84f84a16288be4d11a4d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/8.26k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3737b66031fb43ae92c5f3bc653e1b09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d6ea917e994446db62e63973dbae3b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/420 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1693ad18d864477d89289e040b5524c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(600, 384, padding_idx=1)\n    (position_embeddings): Embedding(515, 384, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 384)\n    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.144, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=384, out_features=384, bias=True)\n            (key): Linear(in_features=384, out_features=384, bias=True)\n            (value): Linear(in_features=384, out_features=384, bias=True)\n            (dropout): Dropout(p=0.109, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=384, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.144, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=384, out_features=464, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=464, out_features=384, bias=True)\n          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.144, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=384, out_features=384, bias=True)\n            (key): Linear(in_features=384, out_features=384, bias=True)\n            (value): Linear(in_features=384, out_features=384, bias=True)\n            (dropout): Dropout(p=0.109, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=384, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.144, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=384, out_features=464, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=464, out_features=384, bias=True)\n          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.144, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=384, out_features=384, bias=True)\n            (key): Linear(in_features=384, out_features=384, bias=True)\n            (value): Linear(in_features=384, out_features=384, bias=True)\n            (dropout): Dropout(p=0.109, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=384, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.144, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=384, out_features=464, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=464, out_features=384, bias=True)\n          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.144, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=384, out_features=384, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "smiles = [\n",
    "    \"[NH]C(CC(C)C([N@@](C(C)(C)C)C(N)(C)N)(C)C)c1c(c(c[nH+][o+]1)C)[O-]\",\n",
    "    \"[NH]C(CC(C)C((C(C)(C)C)CN)C)c1c(c(c[nH+][o+]1)C)[O-]\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[12, 23, 16, 17, 16, 16, 17, 16, 18, 16, 17, 23, 17, 16, 17, 16, 18, 17,\n         16, 18, 16, 18, 16, 17, 23, 18, 17, 16, 18, 23, 18, 17, 16, 18, 16, 18,\n         15, 20, 15, 17, 15, 17, 15, 25, 44, 20, 18, 16, 18, 19, 31, 13],\n        [12, 23, 16, 17, 16, 16, 17, 16, 18, 16, 17, 17, 16, 17, 16, 18, 17, 16,\n         18, 16, 18, 16, 23, 18, 16, 18, 15, 20, 15, 17, 15, 17, 15, 25, 44, 20,\n         18, 16, 18, 19, 31, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0]])}"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_instances = tokenizer(smiles, add_special_tokens=True, return_tensors='pt', padding=True)\n",
    "tokenized_instances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "hidden = model(**tokenized_instances)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4319, -0.1712, -0.0734,  ...,  0.0032,  0.1178, -0.5129],\n         [ 0.0346,  0.3587, -0.0228,  ...,  0.0444, -0.1280,  0.0934],\n         [ 0.2966, -0.1893,  0.0102,  ...,  0.1391, -0.1832, -0.3893],\n         ...,\n         [ 0.1698,  0.0009,  0.2729,  ..., -0.1011,  0.1077, -0.6349],\n         [ 0.2468,  0.2819,  0.3447,  ..., -0.4136, -0.3032, -0.1212],\n         [ 0.5281,  0.0718,  0.3820,  ..., -0.2419, -0.4423, -0.6131]],\n\n        [[ 0.5213, -0.2079, -0.3280,  ..., -0.2234,  0.0519, -0.3463],\n         [ 0.1170,  0.1991, -0.1065,  ...,  0.0228, -0.2716,  0.2028],\n         [ 0.2878, -0.1959, -0.1298,  ..., -0.0672, -0.2435, -0.2075],\n         ...,\n         [ 0.3466,  0.0130,  0.0283,  ..., -0.4068, -0.7194, -0.3947],\n         [-0.0550,  0.1545,  0.0393,  ..., -0.3279, -0.4374, -0.3352],\n         [ 0.0027, -0.0154, -0.2576,  ..., -0.3913, -0.2815, -0.6187]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.0116, -0.1253, -0.0459,  0.0804,  0.0350, -0.0363,  0.1065, -0.1444,\n         -0.0105,  0.0846, -0.1269, -0.0954,  0.0797, -0.0204, -0.0369, -0.0182,\n         -0.1071, -0.1521, -0.0458,  0.0494, -0.0183,  0.1393,  0.1075, -0.1654,\n          0.1244, -0.1453, -0.0218,  0.0268,  0.2086, -0.0550,  0.1611,  0.0564,\n         -0.0424,  0.1422, -0.0484, -0.0453, -0.0892, -0.0201,  0.0054,  0.0611,\n         -0.1330,  0.2413,  0.2427, -0.1191, -0.0558, -0.0392,  0.0958, -0.0318,\n          0.1021,  0.0583,  0.0163, -0.1856,  0.0148, -0.0444,  0.1568, -0.1148,\n          0.2455, -0.0025, -0.1974,  0.0077,  0.0820,  0.0752, -0.0204,  0.0497,\n         -0.0436, -0.2173, -0.2162, -0.1341, -0.1356,  0.0190,  0.1687,  0.1593,\n          0.0315,  0.1060,  0.0326,  0.1257, -0.0338,  0.0110,  0.0431,  0.0320,\n         -0.1047, -0.1682, -0.0673,  0.1489,  0.0657, -0.2306, -0.0518,  0.0966,\n         -0.0144,  0.1447, -0.0944,  0.1025, -0.1020, -0.0779,  0.0672,  0.0227,\n         -0.0321, -0.0464, -0.0094, -0.0405,  0.0423,  0.2773,  0.1274,  0.0314,\n         -0.0218,  0.1045, -0.0527, -0.0483, -0.1030, -0.0416, -0.0040, -0.2127,\n          0.0036, -0.0027, -0.1607,  0.0681,  0.1220,  0.0720, -0.0182, -0.1432,\n          0.1567,  0.0960,  0.1269,  0.1243, -0.2146,  0.0181, -0.0018,  0.0874,\n         -0.0230,  0.2152,  0.1499, -0.1626,  0.0124, -0.0559, -0.0275, -0.0301,\n          0.0281,  0.0419, -0.1335,  0.1378,  0.1254,  0.0095,  0.0277,  0.0488,\n          0.1317, -0.1277,  0.0663,  0.0653,  0.0530, -0.0323,  0.0378, -0.0280,\n          0.0626, -0.0139,  0.1146, -0.0389,  0.2069,  0.1052,  0.2182,  0.1797,\n          0.1393, -0.0144, -0.2570,  0.0401,  0.0377,  0.1708,  0.0359,  0.0991,\n          0.0284,  0.1816,  0.1313, -0.1148,  0.0131,  0.0650, -0.0616,  0.0172,\n         -0.0749, -0.1097,  0.0259, -0.1539, -0.0686, -0.0844,  0.1567,  0.0282,\n          0.0628,  0.0830, -0.0284,  0.0793, -0.1295, -0.0305, -0.0898,  0.0770,\n          0.1258,  0.0724,  0.0482,  0.0589, -0.0220, -0.2295, -0.1226, -0.1557,\n         -0.0474,  0.1741,  0.1602, -0.1290,  0.0490, -0.0019,  0.0179, -0.0459,\n         -0.0300, -0.0026,  0.1151, -0.0198, -0.0528, -0.1320, -0.0062, -0.0450,\n         -0.1446, -0.0022, -0.0435, -0.0685,  0.0322, -0.1895, -0.0314, -0.0674,\n          0.1929, -0.2217,  0.1915,  0.0401, -0.0171, -0.1745, -0.1784,  0.0405,\n         -0.0223,  0.0419,  0.0305, -0.0319, -0.1350,  0.0634, -0.0882, -0.1371,\n          0.0480, -0.1184, -0.1305,  0.1156,  0.0907,  0.1760,  0.0931, -0.1319,\n         -0.0404, -0.1401, -0.1450, -0.0266,  0.0131, -0.1596,  0.0450,  0.0476,\n          0.1319,  0.1275,  0.2199,  0.1788,  0.0349,  0.0287,  0.0947,  0.0880,\n         -0.0241, -0.0377, -0.1675,  0.1340,  0.0648,  0.0247, -0.0462, -0.0351,\n         -0.0521, -0.0690,  0.0807, -0.1163, -0.0293,  0.0648,  0.0468,  0.1513,\n          0.1408, -0.0018,  0.0338,  0.1024, -0.0144, -0.0833,  0.0023, -0.0796,\n          0.0013, -0.0695,  0.0114, -0.1457, -0.0126,  0.0412,  0.1929,  0.0324,\n         -0.0152,  0.1422, -0.0110, -0.0484,  0.0091, -0.1044, -0.0522, -0.1207,\n          0.0414,  0.0442,  0.0378, -0.1290, -0.1256,  0.0601, -0.0211, -0.0888,\n          0.0236,  0.1382, -0.1715,  0.1542,  0.1132,  0.1067, -0.0745,  0.0026,\n         -0.1068, -0.0334, -0.1733, -0.0520, -0.1265, -0.0941, -0.1625, -0.0404,\n         -0.2448, -0.0671,  0.0840, -0.1845,  0.1341,  0.1769,  0.0056, -0.2026,\n          0.1582, -0.0248, -0.1024,  0.1281,  0.0664,  0.0099,  0.0596, -0.1037,\n         -0.0568, -0.1471, -0.1623, -0.1911, -0.0907, -0.0441, -0.1417, -0.1589,\n         -0.1353, -0.0225,  0.0338, -0.0203, -0.1248,  0.0461, -0.1409,  0.0349,\n          0.1673,  0.2265, -0.0486,  0.0158, -0.0103, -0.0216, -0.0298, -0.0672,\n         -0.0326, -0.0531,  0.0998,  0.2178, -0.1724,  0.0601, -0.0335,  0.0317,\n         -0.0797, -0.1334, -0.1381,  0.1728,  0.1491, -0.2537,  0.1627, -0.2622],\n        [-0.0170, -0.0481,  0.0102,  0.1084,  0.0261,  0.0169,  0.1396, -0.0851,\n         -0.0404,  0.0946, -0.0597, -0.0923,  0.1290, -0.0805, -0.0172, -0.0211,\n         -0.0658, -0.0728, -0.0354,  0.0772, -0.0189,  0.0964,  0.1253, -0.1836,\n          0.1479, -0.0574, -0.0032,  0.0341,  0.1657, -0.0133,  0.2026,  0.1087,\n         -0.0443,  0.1018, -0.0144, -0.0994, -0.1416, -0.0424,  0.0020,  0.0495,\n         -0.0932,  0.1655,  0.2341, -0.0524, -0.0436,  0.0116,  0.0833,  0.0170,\n          0.0541,  0.0207, -0.0304, -0.1384, -0.0120, -0.0467,  0.1190, -0.0028,\n          0.1254, -0.0645, -0.1454, -0.0337,  0.0549,  0.1256,  0.0011,  0.0403,\n         -0.0593, -0.2563, -0.1979, -0.0802, -0.1298,  0.1098,  0.1392,  0.1434,\n          0.1056,  0.0243, -0.0587,  0.0462,  0.0140,  0.0157,  0.0555,  0.0860,\n         -0.1010, -0.1530, -0.0646,  0.1585,  0.0205, -0.1504, -0.0166,  0.1267,\n         -0.0152,  0.1264, -0.0074,  0.0729, -0.1027, -0.0741,  0.1000,  0.0617,\n          0.0650, -0.0306, -0.0107, -0.0484,  0.0882,  0.2098,  0.0500,  0.0051,\n         -0.0288,  0.0596, -0.0438, -0.1033, -0.0949, -0.0811, -0.0129, -0.1580,\n          0.0068, -0.0113, -0.1446,  0.0100,  0.1676,  0.1200, -0.0312, -0.1391,\n          0.1774,  0.0569,  0.1593,  0.0511, -0.1477,  0.0014, -0.0072,  0.0719,\n         -0.0723,  0.2540,  0.2114, -0.1504, -0.0338, -0.1048, -0.0592, -0.0704,\n          0.0006,  0.0150, -0.1306,  0.1493,  0.1349,  0.0520, -0.0078,  0.0536,\n          0.0815, -0.1498,  0.0604,  0.1408,  0.0236, -0.0070, -0.0129, -0.0156,\n          0.0127,  0.0051,  0.0867, -0.0254,  0.1534,  0.0964,  0.1834,  0.1645,\n          0.1778, -0.0115, -0.2103,  0.0302,  0.1003,  0.1490, -0.0316,  0.0535,\n          0.0737,  0.1523,  0.0983, -0.1270,  0.0260,  0.1007, -0.0552,  0.0262,\n         -0.0645, -0.1317,  0.0552, -0.1798, -0.1062, -0.0617,  0.1171, -0.0319,\n          0.0515,  0.0695, -0.0670,  0.0178, -0.1026,  0.0370, -0.1664,  0.0359,\n          0.2533,  0.1345,  0.0984,  0.1233, -0.0317, -0.2484, -0.1625, -0.0739,\n         -0.0449,  0.1031,  0.1994, -0.1037, -0.0132,  0.0336, -0.0047, -0.0327,\n          0.0519, -0.0227,  0.0954, -0.0428, -0.0040, -0.0978,  0.0075,  0.0084,\n         -0.0770, -0.0391, -0.1285, -0.0268,  0.0832, -0.1860, -0.0433, -0.0347,\n          0.1299, -0.2356,  0.2256,  0.0281, -0.0120, -0.1778, -0.1507,  0.0662,\n         -0.0835,  0.0424,  0.0596,  0.0189, -0.2151,  0.0465, -0.0738, -0.1376,\n         -0.0031, -0.1046, -0.0981,  0.0870,  0.0855,  0.1129,  0.1025, -0.0729,\n         -0.0204, -0.1457, -0.1403, -0.0449,  0.0479, -0.1149,  0.0252,  0.0921,\n          0.1230,  0.1059,  0.2173,  0.1400, -0.0416,  0.0556,  0.1700,  0.0149,\n         -0.0065, -0.0204, -0.1708,  0.1139,  0.0678,  0.1248,  0.0275,  0.0034,\n         -0.0645, -0.0626,  0.1007, -0.1944, -0.0957,  0.0308, -0.0323,  0.1477,\n          0.2391,  0.0062, -0.0063,  0.1515, -0.0185, -0.0573,  0.0656, -0.0411,\n          0.0589, -0.0633,  0.0836, -0.1354, -0.1273,  0.0519,  0.1550,  0.0135,\n         -0.0418,  0.1283,  0.0592,  0.0004, -0.0052, -0.0887, -0.0478, -0.0920,\n          0.0844,  0.1079,  0.0640,  0.0036, -0.1855,  0.0539, -0.0769, -0.0219,\n          0.0005,  0.1748, -0.1418,  0.1924,  0.1575,  0.1174, -0.0486,  0.0082,\n         -0.0707,  0.0117, -0.1499, -0.0454, -0.1345, -0.0890, -0.1245,  0.0269,\n         -0.2663, -0.0775,  0.0745, -0.2592,  0.1628,  0.1198, -0.0044, -0.1406,\n          0.0527, -0.0463, -0.1093,  0.1633, -0.0305, -0.0402,  0.0993, -0.1419,\n         -0.0710, -0.1342, -0.1690, -0.1915, -0.1141, -0.0779, -0.0720, -0.1687,\n         -0.1072, -0.0055,  0.0714, -0.0167, -0.0715,  0.0694, -0.1093, -0.0085,\n          0.1209,  0.1262, -0.0836, -0.0390,  0.0074,  0.0092,  0.0458, -0.0673,\n         -0.0314, -0.0918,  0.0431,  0.1819, -0.2264,  0.0182,  0.0237,  0.0043,\n         -0.1072, -0.0352, -0.1242,  0.1137,  0.1298, -0.2331,  0.1167, -0.2111]],\n       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 384])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.pooler_output.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 52, 384])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.last_hidden_state.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
