"""
The GROVER models for pretraining, finetuning and fingerprint generating.
"""

from torch import nn as nn
from typing import List, Dict

from .layers import Readout, GTransEncoder
from .utils_nn import get_activation_function
from ..dataset.molgraph import get_atom_fdim, get_bond_fdim


class GROVEREmbedding(nn.Module):
    """
    The GROVER Embedding class. It contains the GTransEncoder.
    This GTransEncoder can be replaced by any validate encoders.
    """

    def __init__(self, args):
        """
        Initialize the GROVEREmbedding class.
        :param args:
        """
        super(GROVEREmbedding, self).__init__()
        self.embedding_output_type = args.embedding_output_type
        edge_dim = get_bond_fdim() + get_atom_fdim()
        node_dim = get_atom_fdim()
        self.encoders = GTransEncoder(args,
                                      hidden_size=args.hidden_size,
                                      edge_fdim=edge_dim,
                                      node_fdim=node_dim,
                                      dropout=args.dropout,
                                      activation=args.activation,
                                      num_mt_block=args.num_mt_block,
                                      num_attn_head=args.num_attn_head,
                                      atom_emb_output=self.embedding_output_type,
                                      bias=args.bias)

    def forward(self, graph_batch: List) -> Dict:
        """
        The forward function takes graph_batch as input and output a dict. The content of the dict is decided by
        self.embedding_output_type.

        :param graph_batch: the input graph batch generated by MolCollator.
        :return: a dict containing the embedding results.
        """
        output = self.encoders(graph_batch)
        if self.embedding_output_type == 'atom':
            return {"atom_from_atom": output[0], "atom_from_bond": output[1],
                    "bond_from_atom": None, "bond_from_bond": None}  # atom_from_atom, atom_from_bond
        elif self.embedding_output_type == 'bond':
            return {"atom_from_atom": None, "atom_from_bond": None,
                    "bond_from_atom": output[0], "bond_from_bond": output[1]}  # bond_from_atom, bond_from_bond
        elif self.embedding_output_type == "both":
            return {"atom_from_atom": output[0][0], "bond_from_atom": output[0][1],
                    "atom_from_bond": output[1][0], "bond_from_bond": output[1][1]}


def create_ffn(config):
    """
    Creates the feed-forward network for the model.

    :param config: Arguments.
    """
    # Note: args.features_dim is set according the real loaded features data
    if config.self_attention:
        first_linear_dim = config.hidden_size * config.attn_out
    else:
        first_linear_dim = config.hidden_size

    dropout = nn.Dropout(config.dropout)
    activation = get_activation_function(config.activation)
    # TODO: ffn_hidden_size
    # Create FFN layers
    if config.ffn_num_layers == 1:
        ffn = [
            dropout,
            nn.Linear(first_linear_dim, config.n_lbs*config.n_tasks)
        ]
    else:
        ffn = [
            dropout,
            nn.Linear(first_linear_dim, config.ffn_hidden_size)
        ]
        for _ in range(config.ffn_num_layers - 2):
            ffn.extend([
                activation,
                dropout,
                nn.Linear(config.ffn_hidden_size, config.ffn_hidden_size),
            ])
        ffn.extend([
            activation,
            dropout,
            nn.Linear(config.ffn_hidden_size, config.n_lbs*config.n_tasks),
        ])

    # Create FFN model
    return nn.Sequential(*ffn)


class GROVERFinetuneModel(nn.Module):
    """
    The finetune
    """
    def __init__(self, config):
        super(GROVERFinetuneModel, self).__init__()

        self.hidden_size = config.hidden_size

        self.grover = GROVEREmbedding(config)

        if config.self_attention:
            self.readout = Readout(rtype="self_attention", hidden_size=self.hidden_size,
                                   attn_hidden=config.attn_hidden,
                                   attn_out=config.attn_out)
        else:
            self.readout = Readout(rtype="mean", hidden_size=self.hidden_size)

        self.mol_atom_from_atom_ffn = create_ffn(config)
        self.mol_atom_from_bond_ffn = create_ffn(config)

    @staticmethod
    def get_loss_func(args):
        def loss_func(preds, targets,
                      dt=args.dataset_type,
                      dist_coff=args.dist_coff):

            if dt == 'classification':
                pred_loss = nn.BCEWithLogitsLoss(reduction='none')
            elif dt == 'regression':
                pred_loss = nn.MSELoss(reduction='none')
            else:
                raise ValueError(f'Dataset type "{args.dataset_type}" not supported.')

            if type(preds) is not tuple:
                # in eval mode.
                return pred_loss(preds, targets)

            # in train mode.
            dist_loss = nn.MSELoss(reduction='none')

            dist = dist_loss(preds[0], preds[1])
            pred_loss1 = pred_loss(preds[0], targets)
            pred_loss2 = pred_loss(preds[1], targets)
            return pred_loss1 + pred_loss2 + dist_coff * dist

        return loss_func

    def forward(self, batch, **kwargs):
        molecule_components = batch.molecule_graphs.components
        _, _, _, _, _, a_scope, _, _ = molecule_components

        output = self.grover(molecule_components)
        # Share readout
        mol_atom_from_bond_output = self.readout(output["atom_from_bond"], a_scope)
        mol_atom_from_atom_output = self.readout(output["atom_from_atom"], a_scope)

        atom_ffn_output = self.mol_atom_from_atom_ffn(mol_atom_from_atom_output)
        bond_ffn_output = self.mol_atom_from_bond_ffn(mol_atom_from_bond_output)
        return atom_ffn_output, bond_ffn_output
